{% extends "dailyactivities/layout.html"%}

{% block heading %}
  <title>Requirements page</title>
{% endblock heading%}

{% block content %}

<h1><b>MACHINE LEARNING SKILLS</b></h1>
<p><font size="2">
<ol>

<li><h3> ML CLASSES</h3></li>
<ol>
    <li><b>Unsupervised Learning</b></li>
        <ul><li></li></ul>
    <li><b>Supervised Learning</b></li>
        <ul><li></li></ul>
    <li><b>Deep learning</b></li>
        <ul><li></li></ul>
    <li><b>Machine learning</b></li>
        <ul><li></li></ul>
    <li><b>Cognitive science</b></li>
        <ul><li></li></ul>
    <li><b>Analytics</b></li>    
        <ul><li></li></ul>
</ol><br>

<li><h3> DEEP LEARNING & LIBRARIES</h3></li>
<ol>
    <li><b>Robotics</b></li>
        <ul><li></li></ul>
    <li><b>Deep learning</b></li>
        <ul><li></li></ul>
    <li><b>Machine Learning</b></li>
        <ul><li></li></ul>
    <li><b>Deep Learning</b></li>
        <ul><li></li></ul>
    <li><b>Caffe</b></li>
        <ul><li></li></ul>
    <li><b>TensorFlow</b></li>
        <ul><li></li></ul>
    <li><b>Theano</b></li>
        <ul><li></li></ul>
    <li><b>Torch</b></li>
        <ul><li></li></ul>
    <li><b>research/publications on machine learning</b></li>
        <ul><li></li></ul>
    <li><b>artificial intelligence</b></li>
        <ul><li></li></ul>
    <li><b>AB testing</b></li>
        <ul><li></li></ul>
    <li><b>Keras</b></li>
        <ul><li></li></ul>
</ol><br>

<li><h3> BIG DATA TOOLS </h3></li>
<ol>
    <li><b>Hadoop</b></li>
        <p align="justify"><ul><li>Apache Hadoop ( /həˈduːp/) is a collection of open-source software utilities that facilitate using a network of many computers to solve problems involving massive amounts of data and computation. It provides a software framework for distributed storage and processing of big data using the MapReduce programming model. Originally designed for computer clusters built from commodity hardware[3]—still the common use—it has also found use on clusters of higher-end hardware.[4][5] All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common occurrences and should be automatically handled by the framework.[6]</li></ul></p>
    <li><b>MapReduce</b></li>
        <p align="justify"><ul><li>MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster.A MapReduce program is composed of a map procedure, which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method, which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The "MapReduce System" (also called "infrastructure" or "framework") orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance.</li></ul></p>
    <li><b>HBase</b></li>
        <p align="justify"><ul><li>HBase is a data model that is similar to Google’s big table designed to provide quick random access to huge amounts of structured data. This tutorial provides an introduction to HBase, the procedures to set up HBase on Hadoop File Systems, and ways to interact with HBase shell. It also describes how to connect to HBase using java, and how to perform basic operations on HBase using java. </li></ul></p>
    <li><b>Pig</b></li>
        <p align="justify"><ul><li> Apache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. The salient property of Pig programs is that their structure is amenable to substantial parallelization, which in turns enables them to handle very large data sets.At the present time, Pig's infrastructure layer consists of a compiler that produces sequences of Map-Reduce programs, for which large-scale parallel implementations already exist (e.g., the Hadoop subproject). Pig's language layer currently consists of a textual language called Pig Latin, which has the following key properties:Ease of programming. It is trivial to achieve parallel execution of simple, "embarrassingly parallel" data analysis tasks. Complex tasks comprised of multiple interrelated data transformations are explicitly encoded as data flow sequences, making them easy to write, understand, and maintain.Optimization opportunities. The way in which tasks are encoded permits the system to optimize their execution automatically, allowing the user to focus on semantics rather than efficiency.Extensibility. Users can create their own functions to do special-purpose processing.</li></ul></p>
    <li><b>Hive</b></li>
        <p align="justify"><ul><li>Apache Hive is a data warehouse software project built on top of Apache Hadoop for providing data query and analysis. Hive gives a SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop. Traditional SQL queries must be implemented in the MapReduce Java API to execute SQL applications and queries over distributed data. Hive provides the necessary SQL abstraction to integrate SQL-like queries (HiveQL) into the underlying Java without the need to implement queries in the low-level Java API. Since most data warehousing applications work with SQL-based querying languages, Hive aids portability of SQL-based applications to Hadoop. While initially developed by Facebook, Apache Hive is used and developed by other companies such as Netflix and the Financial Industry Regulatory Authority (FINRA). Amazon maintains a software fork of Apache Hive included in Amazon Elastic MapReduce on Amazon Web Services.</li></ul></p>
    <li><b>Impala</b></li>
        <p align="justify"><ul><li>Impala raises the bar for SQL query performance on Apache Hadoop while retaining a familiar user experience. With Impala, you can query data, whether stored in HDFS or Apache HBase – including SELECT, JOIN, and aggregate functions – in real time. Furthermore, Impala uses the same metadata, SQL syntax (Hive SQL), ODBC driver, and user interface (Hue Beeswax) as Apache Hive, providing a familiar and unified platform for batch-oriented or real-time queries. (For that reason, Hive users can utilize Impala with little setup overhead.)</li></ul></p>
    <li><b>Spark Scala</b></li>
        <p align="justify"><ul><li>At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.</li></ul></p>
    <li><b>Sqoop</b></li>
        <p align="justify"><ul><li>Apache Sqoop is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases. You can use Sqoop to import data from external structured datastores into Hadoop Distributed File System or related systems like Hive and HBase. Conversely, Sqoop can be used to extract data from Hadoop and export it to external structured datastores such as relational databases and enterprise data warehouses.In its monthly meeting in March of 2012, the board of Apache Software Foundation (ASF) resolved to grant a Top-Level Project status to Apache Sqoop, thus graduating it from the Incubator. This is a significant milestone in the life of Sqoop, which has come a long way since its inception almost three years ago. The following figure offers a brief overview of what has happened in the life of Sqoop so far:</li></ul></p>
    <li><b>Flume</b></li>
        <p align="justify"><ul><li>Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application.</li></ul></p>
    <li><b>Kafka</b></li>
        <p align="justify"><ul><li>Apache Kafka is an open-source stream-processing software platform developed by LinkedIn and donated to the Apache Software Foundation, written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Kafka can connect to external systems (for data import/export) via Kafka Connect and provides Kafka Streams, a Java stream processing library. Kafka uses a binary TCP-based protocol that is optimized for efficiency and relies on a "message set" abstraction that naturally groups messages together to reduce the overhead of the network roundtrip. This "leads to larger network packets, larger sequential disk operations, contiguous memory blocks [...] which allows Kafka to turn a bursty stream of random message writes into linear writes."</li></ul></p>
    <li><b>HDFS</b></li>
        <p align="justify"><ul><li>The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS relaxes a few POSIX requirements to enable streaming access to file system data. HDFS was originally built as infrastructure for the Apache Nutch web search engine project. HDFS is now an Apache Hadoop subproject. The project URL is https://hadoop.apache.org/hdfs/.</li></ul></p>
</ol><br>

<li><h3> IMAGE PROCESSING </h3></li>
<ol>
    <li><b>Vision systems</b></li>
    <li><b>Image Classification, </b></li>
    <li><b>Object Detection</b></li>
    <li><b>Semantic Segmentation</b></li>
</ol><br>

<li><h3> STATISTICAL SKILLS </h3></li>
<ol>
    <li><b>Business models</b></li> 
        <ul><li></li></ul>
    <li><b>Operating models</b></li>
        <ul><li></li></ul>
    <li><b>Financial models</b></li>
        <ul><li></li></ul>
    <li><b>Cost-benefit analysis</b></li>
        <ul><li></li></ul>
    <li><b>Budgeting and risk management</b></li>
        <ul><li></li></ul>
    <li><b>Predictive analytics</b></li> 
        <p align="justify"><ul><li>Predictive analytics encompasses a variety of statistical techniques from data mining, predictive modelling, and machine learning, that analyze current and historical facts to make predictions about future or otherwise unknown events.In business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions.</li></ul></p>
    <li><b>Bayesian modeling</b></li>
        <p align="justify"><ul><li>Bayesian decision theory refers to a decision theory which is informed by Bayesian probability. It is a statistical system that tries to quantify the tradeoff between various decisions, making use of probabilities and costs. An agent operating under such a decision theory uses the concepts of Bayesian statistics to estimate the expected value of its actions, and update its expectations based on new information. These agents can and are usually referred to as estimators.</li></ul></p>
    <li><b>Gaussian processes</b></li>
        <ul><li></li></ul>
    <li><b>Random forests</b></li>
        <ul><li></li></ul>
    <li><b>Boosting trees</b></li>
        <ul><li></li></ul>
    <li><b>Advanced visualization</b></li>
        <ul><li></li></ul>
    <li><b>R</b></li>
        <ul><li></li></ul>
    <li><b>clustering</b></li>
        <ul><li></li></ul>
    <li><b>decision trees</b></li>
        <ul><li></li></ul>
</ol><br>


<li><h3> NLP </h3></li>
<ol>
    <li><b>NLU</b></li>
        <ul><li></li></ul>
    <li><b>NL classical parsing techniques</b></li>
        <ul><li></li></ul>
    <li><b>Word Vectorizers</b></li>
        <ul><li></li></ul>
    
    <li><b>Word Embeddings ( word2vec & GloVe )</b></li>

        <p align="justify"><ul><li>Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension.Methods to generate this mapping include neural networks,[1] dimensionality reduction on the word co-occurrence matrix,[2][3][4] probabilistic models,[5] explainable knowledge base method,[6] and explicit representation in terms of the context in which words appear.[7]</li></ul>
    
    <li><b><a href="{% url 'dailyactivities-RNN'%}">RNN</a> ( CNN vs RNN )</b></li></p>

        <p align="justify"><ul><li>RNNs are specialized neural-based approaches that are effective at processing sequential information. An RNN recursively applies a computation to every instance of an input sequence conditioned on the previous computed results. These sequences are typically represented by a fixed-size vector of tokens which are fed sequentially (one by one) to a recurrent unit. The figure below illustrates a simple RNN framework below.</li></ul></p>
    <li><b>LSTM & GRU ( LSTM vs GRU )</b></li>
        
        <p align="justify"><ul><li>Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or IDS's (intrusion detection systems).</li></ul></p>
    
    <li><b>GRU</b></li>
        <p align="justify"><ul><li>Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with forget gate but has fewer parameters than LSTM, as it lacks an output gate. GRU's performance on certain tasks of polyphonic music modeling and speech signal modeling was found to be similar to that of LSTM. GRUs have been shown to exhibit even better performance on certain smaller datasets.</li></ul></p>

    <li><b>designing & building NLP conversational systems</b></li>
        <ul><li></li></ul>
    <li><b>Auto Encoders</b></li>
        <p align="justify"><ul><li>Autoencoders (AE) are neural networks that aims to copy their inputs to their outputs. They work by compressing the input into a latent-space representation, and then reconstructing the output from this representation. This kind of network is composed of two parts :
Encoder: This is the part of the network that compresses the input into a latent-space representation. It can be represented by an encoding function h=f(x).
Decoder: This part aims to reconstruct the input from the latent space representation. It can be represented by a decoding function r=g(h).</li></ul></p>
    <li><b>Dialog systems</b></li>
        <p align="justify"><ul><li>A dialogue system, or conversational agent (CA), is a computer system intended to converse with a human. Dialogue systems employed one or more of text, speech, graphics, haptics, gestures, and other modes for communication on both the input and output channel.</li></ul></p>
    <li><b>time-series analysis</b></li>
        <ul><li></li></ul>
    <li><b>Pretrained Embeddings ( Implementation in RNN )</b></li>
        <ul><li></li></ul>
    <li><b>Keras Embedding Layer output</b></li>
        <ul><li></li></ul>
    <li><b>Text Clustering</b></li>
        <ul><li></li></ul>
    <li><b>Topic Modelling</b></li>
        <p align="justify"><ul><li>In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.</li></ul>
    <li><b>Information Extraction</b></li></p>
        <ul><li></li></ul>
    <li><b>Information Retrieval</b></li>
        <ul><li></li></ul>
</ol><br>

</ol>
</font>
</p>

<h1> SOFT SKILLS </h1>
<p><font size="2">
<ol>
<li>Strong leadership skills</li>
<li>analytical and technical skills</li>
<li>graphical modeling approaches, tools and model repositories</li>
<li>functions and capabilities of new technologies</li>
<li>organization's digital business strategies</li>
<li>Understand and speak the language of the business</li>
<li>Influential in the organization and a team player</li>
<li>Effective at driving short-term actions that are consistent with long-term goals</li>
<li>Trusted and respected as a thought leader who can influence and persuade business and IT leaders</li>
<li>Comfortable, experienced and accomplished at working with business executives, and able to push back in a professional and diplomatic way</li>
<li>Highly collaborative and supportive of business and of its ideals and strategies</li>
</ol>
</font>
</p>


<h1> CLOUD SKILLS </h1>
<p><font size="2">
<ol>
<li>IOT</li>
<li>SaaS</li>
<li>Infrastructure as a service (IaaS)</li>
<li>platform as a service (PaaS)</li>
<li>SOA</li>
<li>APIs</li>
<li>open data</li>
<li>micro-services</li>
<li>event-driven IT</li>
<li>retail commerce</li>
<li>business ecosystems</li>
</ol>
</font>
</p>
{% endblock content %}