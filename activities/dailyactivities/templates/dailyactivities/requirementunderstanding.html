{% extends "dailyactivities/layout.html"%}

{% block heading %}
  <title>Requirements page</title>
{% endblock heading%}

{% block content %}

<h1><b>MACHINE LEARNING SKILLS</b></h1>
<p><font size="2">
<ol>

<li><h3> ML CLASSES</h3></li>
<ol>
    <li><b>Unsupervised Learning</b></li>
        <ul><li></li></ul>
    <li><b>Supervised Learning</b></li>
        <ul><li></li></ul>
    <li><b>Deep learning</b></li>
        <ul><li></li></ul>
    <li><b>Machine learning</b></li>
        <ul><li></li></ul>
    <li><b>Cognitive science</b></li>
        <ul><li></li></ul>
    <li><b>Analytics</b></li>    
        <ul><li></li></ul>
</ol><br>

<li><h3> DEEP LEARNING & LIBRARIES</h3></li>
<ol>
    <li><b>Deep Learning</b></li>
        <ul><li>Deep learning (also known as deep structured learning or differential programming) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.Deep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.</li></ul>
    <li><b>Robotics</b></li>
        <ul><li></li></ul>
    <li><b>Machine Learning</b></li>
        <ul><li></li></ul>
    <li><b>Deep Learning</b></li>
        <ul><li></li></ul>
    <li><b>Caffe</b></li>
        <p align="justify"><ul><li>Caffe is a deep learning framework made with expression, speed, and modularity in mind. It is developed by Berkeley AI Research (BAIR) and by community contributors. Yangqing Jia created the project during his PhD at UC Berkeley. Caffe is released under the BSD 2-Clause license. </li></ul></p>
    <li><b>TensorFlow</b></li>
        <p align="justify"><ul><li>TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.</li></ul></p>
    <li><b>Theano</b></li>
        <p align="justify"><ul><li>Theano is a Python library and optimizing compiler for manipulating and evaluating mathematical expressions, especially matrix-valued ones.[2] In Theano, computations are expressed using a NumPy-esque syntax and compiled to run efficiently on either CPU or GPU architectures.Theano is an open source project[3] primarily developed by a Montreal Institute for Learning Algorithms (MILA) at the Université de Montréal.[4]The name of the software references the ancient philosopher Theano, long associated with the development of the golden mean.On 28 September 2017, Pascal Lamblin posted a message from Yoshua Bengio, Head of MILA: major development would cease after the 1.0 release due to competing offerings by strong industrial players.[5] Theano 1.0.0 was then released on 15 November 2017.[6]On 17 May 2018, Chris Fonnesbeck wrote on behalf of the PyMC development team[7] that the PyMC developers will officially assume control of Theano maintenance once they step down. </li></ul></p>
    <li><b>Torch</b></li>
        <p align="justify"><ul><li>TorchScript provides a seamless transition between eager mode and graph mode to accelerate the path to production.</li></ul></p>
    <li><b>research/publications on machine learning</b></li>
        <ul><li></li></ul>
    <li><b>artificial intelligence</b></li>
        <ul><li></li></ul>
    <li><b>AB testing</b></li>
        <ul><li></li></ul>
    <li><b>Keras</b></li>
        <p align="justify"><ul><li>Keras is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, R, Theano, or PlaidML. Designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible. It was developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System), and its primary author and maintainer is François Chollet, a Google engineer. Chollet also is the author of the XCeption deep neural network model.In 2017, Google's TensorFlow team decided to support Keras in TensorFlow's core library.Chollet explained that Keras was conceived to be an interface rather than a standalone machine learning framework. It offers a higher-level, more intuitive set of abstractions that make it easy to develop deep learning models regardless of the computational backend used.Microsoft added a CNTK backend to Keras as well, available as of CNTK v2.0.</li></ul></p>
</ol><br>

<li><h3> BIG DATA TOOLS </h3></li>
<ol>
    <li><b>Hadoop</b></li>
        <p align="justify"><ul><li>Apache Hadoop ( /həˈduːp/) is a collection of open-source software utilities that facilitate using a network of many computers to solve problems involving massive amounts of data and computation. It provides a software framework for distributed storage and processing of big data using the MapReduce programming model. Originally designed for computer clusters built from commodity hardware[3]—still the common use—it has also found use on clusters of higher-end hardware.[4][5] All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common occurrences and should be automatically handled by the framework.[6]</li></ul></p>
    <li><b>MapReduce</b></li>
        <p align="justify"><ul><li>MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster.A MapReduce program is composed of a map procedure, which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method, which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The "MapReduce System" (also called "infrastructure" or "framework") orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance.</li></ul></p>
    <li><b>HBase</b></li>
        <p align="justify"><ul><li>HBase is a data model that is similar to Google’s big table designed to provide quick random access to huge amounts of structured data. This tutorial provides an introduction to HBase, the procedures to set up HBase on Hadoop File Systems, and ways to interact with HBase shell. It also describes how to connect to HBase using java, and how to perform basic operations on HBase using java. </li></ul></p>
    <li><b>Pig</b></li>
        <p align="justify"><ul><li> Apache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. The salient property of Pig programs is that their structure is amenable to substantial parallelization, which in turns enables them to handle very large data sets.At the present time, Pig's infrastructure layer consists of a compiler that produces sequences of Map-Reduce programs, for which large-scale parallel implementations already exist (e.g., the Hadoop subproject). Pig's language layer currently consists of a textual language called Pig Latin, which has the following key properties:Ease of programming. It is trivial to achieve parallel execution of simple, "embarrassingly parallel" data analysis tasks. Complex tasks comprised of multiple interrelated data transformations are explicitly encoded as data flow sequences, making them easy to write, understand, and maintain.Optimization opportunities. The way in which tasks are encoded permits the system to optimize their execution automatically, allowing the user to focus on semantics rather than efficiency.Extensibility. Users can create their own functions to do special-purpose processing.</li></ul></p>
    <li><b>Hive</b></li>
        <p align="justify"><ul><li>Apache Hive is a data warehouse software project built on top of Apache Hadoop for providing data query and analysis. Hive gives a SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop. Traditional SQL queries must be implemented in the MapReduce Java API to execute SQL applications and queries over distributed data. Hive provides the necessary SQL abstraction to integrate SQL-like queries (HiveQL) into the underlying Java without the need to implement queries in the low-level Java API. Since most data warehousing applications work with SQL-based querying languages, Hive aids portability of SQL-based applications to Hadoop. While initially developed by Facebook, Apache Hive is used and developed by other companies such as Netflix and the Financial Industry Regulatory Authority (FINRA). Amazon maintains a software fork of Apache Hive included in Amazon Elastic MapReduce on Amazon Web Services.</li></ul></p>
    <li><b>Impala</b></li>
        <p align="justify"><ul><li>Impala raises the bar for SQL query performance on Apache Hadoop while retaining a familiar user experience. With Impala, you can query data, whether stored in HDFS or Apache HBase – including SELECT, JOIN, and aggregate functions – in real time. Furthermore, Impala uses the same metadata, SQL syntax (Hive SQL), ODBC driver, and user interface (Hue Beeswax) as Apache Hive, providing a familiar and unified platform for batch-oriented or real-time queries. (For that reason, Hive users can utilize Impala with little setup overhead.)</li></ul></p>
    <li><b>Spark Scala</b></li>
        <p align="justify"><ul><li>At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.</li></ul></p>
    <li><b>Sqoop</b></li>
        <p align="justify"><ul><li>Apache Sqoop is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases. You can use Sqoop to import data from external structured datastores into Hadoop Distributed File System or related systems like Hive and HBase. Conversely, Sqoop can be used to extract data from Hadoop and export it to external structured datastores such as relational databases and enterprise data warehouses.In its monthly meeting in March of 2012, the board of Apache Software Foundation (ASF) resolved to grant a Top-Level Project status to Apache Sqoop, thus graduating it from the Incubator. This is a significant milestone in the life of Sqoop, which has come a long way since its inception almost three years ago. The following figure offers a brief overview of what has happened in the life of Sqoop so far:</li></ul></p>
    <li><b>Flume</b></li>
        <p align="justify"><ul><li>Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. It has a simple and flexible architecture based on streaming data flows. It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application.</li></ul></p>
    <li><b>Kafka</b></li>
        <p align="justify"><ul><li>Apache Kafka is an open-source stream-processing software platform developed by LinkedIn and donated to the Apache Software Foundation, written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Kafka can connect to external systems (for data import/export) via Kafka Connect and provides Kafka Streams, a Java stream processing library. Kafka uses a binary TCP-based protocol that is optimized for efficiency and relies on a "message set" abstraction that naturally groups messages together to reduce the overhead of the network roundtrip. This "leads to larger network packets, larger sequential disk operations, contiguous memory blocks [...] which allows Kafka to turn a bursty stream of random message writes into linear writes."</li></ul></p>
    <li><b>HDFS</b></li>
        <p align="justify"><ul><li>The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS relaxes a few POSIX requirements to enable streaming access to file system data. HDFS was originally built as infrastructure for the Apache Nutch web search engine project. HDFS is now an Apache Hadoop subproject. The project URL is https://hadoop.apache.org/hdfs/.</li></ul></p>
</ol><br>

<li><h3> IMAGE PROCESSING </h3></li>
<ol>
    <li><b>Vision systems</b></li>
        <p align="justify"><ul><li>The visual system is the part of the central nervous system which gives organisms the ability to process visual detail as sight, as well as enabling the formation of several non-image photo response functions. It detects and interprets information from visible light to build a representation of the surrounding environment. The visual system carries out a number of complex tasks, including the reception of light and the formation of monocular representations; the buildup of a nuclear binocular perception from a pair of two dimensional projections; the identification and categorization of visual objects; assessing distances to and between objects; and guiding body movements in relation to the objects seen. The psychological process of visual information is known as visual perception, a lack of which is called blindness. Non-image forming visual functions, independent of visual perception, include the pupillary light reflex (PLR) and circadian photoentrainment. </li></ul></p>
    <li><b>Image Classification, </b></li>
        <p align="justify"><ul><li>Image classification refers to a process in computer vision that can classify an image according to its visual content. For example, an image classification algorithm may be designed to tell if an image contains a human figure or not. While detecting an object is trivial for humans, robust image classification is still a challenge in computer vision applications.</li></ul></p> 
    <li><b>Object Detection</b></li>
        <p align="justify"><ul><li>Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. Well-researched domains of object detection include face detection and pedestrian detection. Object detection has applications in many areas of computer vision, including image retrieval and video surveillance. </li></ul></p>
    <li><b>Semantic Segmentation</b></li>
        <p align="justify"><ul><li>In computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as image objects). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze.[1][2] Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.The result of image segmentation is a set of segments that collectively cover the entire image, or a set of contours extracted from the image (see edge detection). Each of the pixels in a region are similar with respect to some characteristic or computed property, such as color, intensity, or texture. Adjacent regions are significantly different with respect to the same characteristic(s).[1] When applied to a stack of images, typical in medical imaging, the resulting contours after image segmentation can be used to create 3D reconstructions with the help of interpolation algorithms like Marching cubes.[3]</li></ul></p> 
</ol><br>

<li><h3> STATISTICAL SKILLS </h3></li>
<ol>
    <li><b>Business models</b></li> 
        <ul><li></li></ul>
    <li><b>Operating models</b></li>
        <ul><li></li></ul>
    <li><b>Financial models</b></li>
        <ul><li></li></ul>
    <li><b>Cost-benefit analysis</b></li>
        <ul><li></li></ul>
    <li><b>Budgeting and risk management</b></li>
        <ul><li></li></ul>
    <li><b>Predictive analytics</b></li> 
        <p align="justify"><ul><li>Predictive analytics encompasses a variety of statistical techniques from data mining, predictive modelling, and machine learning, that analyze current and historical facts to make predictions about future or otherwise unknown events.In business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions.</li></ul></p>
    <li><b>Bayesian modeling</b></li>
        <p align="justify"><ul><li>Bayesian decision theory refers to a decision theory which is informed by Bayesian probability. It is a statistical system that tries to quantify the tradeoff between various decisions, making use of probabilities and costs. An agent operating under such a decision theory uses the concepts of Bayesian statistics to estimate the expected value of its actions, and update its expectations based on new information. These agents can and are usually referred to as estimators.</li></ul></p>
    <li><b>Gaussian processes</b></li>
        <ul><li></li></ul>
    <li><b>Random forests</b></li>
        <ul><li></li></ul>
    <li><b>Boosting trees</b></li>
        <ul><li></li></ul>
    <li><b>Advanced visualization</b></li>
        <ul><li></li></ul>
    <li><b>R</b></li>
        <ul><li></li></ul>
    <li><b>clustering</b></li>
        <ul><li></li></ul>
    <li><b>decision trees</b></li>
        <ul><li></li></ul>
</ol><br>


<li><h3> NLP </h3></li>
<ol>
    <li><b>NLU</b></li>
        <ul><li></li></ul>
    <li><b>NL classical parsing techniques</b></li>
        <ul><li></li></ul>
    <li><b>Word Vectorizers</b></li>
        <ul><li></li></ul>
    
    <li><b>Word Embeddings ( word2vec & GloVe )</b></li>

        <p align="justify"><ul><li>Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension.Methods to generate this mapping include neural networks,[1] dimensionality reduction on the word co-occurrence matrix,[2][3][4] probabilistic models,[5] explainable knowledge base method,[6] and explicit representation in terms of the context in which words appear.[7]</li></ul>
    
    <li><b><a href="{% url 'dailyactivities-RNN'%}">RNN</a> ( CNN vs RNN )</b></li></p>

        <p align="justify"><ul><li>RNNs are specialized neural-based approaches that are effective at processing sequential information. An RNN recursively applies a computation to every instance of an input sequence conditioned on the previous computed results. These sequences are typically represented by a fixed-size vector of tokens which are fed sequentially (one by one) to a recurrent unit. The figure below illustrates a simple RNN framework below.</li></ul></p>
    <li><b>LSTM & GRU ( LSTM vs GRU )</b></li>
        
        <p align="justify"><ul><li>Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or IDS's (intrusion detection systems).</li></ul></p>
    
    <li><b>GRU</b></li>
        <p align="justify"><ul><li>Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with forget gate but has fewer parameters than LSTM, as it lacks an output gate. GRU's performance on certain tasks of polyphonic music modeling and speech signal modeling was found to be similar to that of LSTM. GRUs have been shown to exhibit even better performance on certain smaller datasets.</li></ul></p>

    <li><b>designing & building NLP conversational systems</b></li>
        <ul><li></li></ul>
    <li><b>Auto Encoders</b></li>
        <p align="justify"><ul><li>Autoencoders (AE) are neural networks that aims to copy their inputs to their outputs. They work by compressing the input into a latent-space representation, and then reconstructing the output from this representation. This kind of network is composed of two parts :
Encoder: This is the part of the network that compresses the input into a latent-space representation. It can be represented by an encoding function h=f(x).
Decoder: This part aims to reconstruct the input from the latent space representation. It can be represented by a decoding function r=g(h).</li></ul></p>
    <li><b>Dialog systems</b></li>
        <p align="justify"><ul><li>A dialogue system, or conversational agent (CA), is a computer system intended to converse with a human. Dialogue systems employed one or more of text, speech, graphics, haptics, gestures, and other modes for communication on both the input and output channel.</li></ul></p>
    <li><b>time-series analysis</b></li>
        <ul><li></li></ul>
    <li><b>Pretrained Embeddings ( Implementation in RNN )</b></li>
        <ul><li></li></ul>
    <li><b>Keras Embedding Layer output</b></li>
        <ul><li></li></ul>
    <li><b>Text Clustering</b></li>
        <ul><li></li></ul>
    <li><b>Topic Modelling</b></li>
        <p align="justify"><ul><li>In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.</li></ul>
    <li><b>Information Extraction</b></li></p>
        <ul><li></li></ul>
    <li><b>Information Retrieval</b></li>
        <ul><li></li></ul>
</ol><br>

</ol>
</font>
</p>

<h1> SOFT SKILLS </h1>
<p><font size="2">
<ol>
<li>Strong leadership skills</li>
<li>analytical and technical skills</li>
<li>graphical modeling approaches, tools and model repositories</li>
<li>functions and capabilities of new technologies</li>
<li>organization's digital business strategies</li>
<li>Understand and speak the language of the business</li>
<li>Influential in the organization and a team player</li>
<li>Effective at driving short-term actions that are consistent with long-term goals</li>
<li>Trusted and respected as a thought leader who can influence and persuade business and IT leaders</li>
<li>Comfortable, experienced and accomplished at working with business executives, and able to push back in a professional and diplomatic way</li>
<li>Highly collaborative and supportive of business and of its ideals and strategies</li>
</ol>
</font>
</p>


<h1> CLOUD SKILLS </h1>
<p><font size="2">
<ol>
<li>IOT</li>
<li>SaaS</li>
<li>Infrastructure as a service (IaaS)</li>
<li>platform as a service (PaaS)</li>
<li>SOA</li>
<li>APIs</li>
<li>open data</li>
<li>micro-services</li>
<li>event-driven IT</li>
<li>retail commerce</li>
<li>business ecosystems</li>
</ol>
</font>
</p>


Submission rules

Before submitting your article, there are a few essential things you need to know. Make sure you read each point well, and that you understand them, as by submitting an article to TDS, you are agreeing to comply with all of them.

    Medium’s Rules and Terms of Service apply to Towards Data Science (TDS) as it is a Medium publication. Make sure you read it before submitting your article.
    As explained in Medium’s Terms of Service, you own the rights to the content you create and post on Medium and therefore Towards Data Science. You’re also responsible for the content you post. This means you assume all risks related to it, including someone else’s reliance on its accuracy, or claims relating to intellectual property or other legal rights.
    We have adopted Medium’s Content Guidelines and Medium’s Curation Guidelines for every article we publish. This means that if your post isn’t of a high enough quality to be curated or doesn’t follow the guidelines, we won’t publish it on Towards Data Science.
    Please don’t submit more than three stories in a single day. Any more than three submissions will be refused without reading. You’re welcome to resubmit them in the future.
    You can edit and remove your article from our publication at any time.
    We might directly edit your content to correct basic spelling mistakes and update minimal formatting. Also, we might remove images where the source isn’t clearly stated. Copyright violation is a real thing and could happen to you. It is your responsibility to ensure you own or have a legal right to use, all content, images, and videos you include in your articles.
    We can remove any articles you post on Towards Data Science for any reason. If we do so, your content will not be lost but still hosted on Medium.com and redirected there.
    If our editorial team finds one or more violations of our rules, we can remove you and all of your articles from our publication and report them to Medium.

Guidelines

How to get your article ready for publication!

We aim to strike a balance between innovating, informing and philosophizing. We want to hear from you! But we do ask that if you are not a professional writer, you consider the following points when you prepare your article. We want to publish high quality, professional articles that people want to read.
1. Is your story a story that needs to be told?

Before you start writing, ask yourself: is this story a story that needs to be told?

If you have read many articles addressing the same issue or explaining the same concept, think twice before writing another one. If you have a radical, new take on an old chestnut, we want to hear from you… but, we need you to persuade us that your article is something special that distinguishes itself from the pack and speaks to our audience.

Conversely, if your article addresses an underserved area or presents a new idea or method, that’s just what we are after!
2. What is your message?

Let us know what your ‘take home message’ is, right from the start. Give your piece a snappy introduction that tells us:

    What is your novel idea?
    Why should we care?
    How are you going to prove your point?

Once you’ve got that out of the way, you can be as conversational as you like, but keep calling back to the central message and give us a solid conclusion.

Remember though, Towards Data Science is not your personal blog, keep it sharp and on-topic!
3. On the internet, nobody knows you are a dog

You’ve got a new idea or a new way of doing things, you want to tell the community and start a discussion. Fantastic, that’s what we want too, but we’re not going to take for granted that you know what you are talking about or that we should uncritically believe what you say… you’ve got to persuade us (your audience) that:

    The subject matter is important
    There is a gap that needs to be filled
    You have the answer
    Your solution works
    Your idea is based on a logical progression of ideas and evidence
    If you are giving us a tutorial, tell us why people would need to use this tool and why your way is better than the methods already published.

You can do this by explaining the background, showing examples, providing an experiment or just laying out how data you have extracted from various sources allowed you to synthesise this new idea.

Are there arguments that counter your opinion or your findings? Explain why that interpretation conflicts with your idea and why your idea comes out on top.
4. Do you have a short title with an insightful subtitle?

If you scroll up this page you will see what a title and subtitle look like. Your post also needs to have a short title and a longer subtitle that tell potential readers why they should read your post. Overall, your header is useful for making your aims clear to readers.

Also, when your subtitle is directly under the headline and it’s formatted as H2 it will show up in some post previews which helps with click-through rate.
5. What value do you deliver to your readers?

Make sure your post achieves its purpose. For instance, if your goal is to explain several concepts, ensure that these concepts are bared to their essentials, narrated in an orderly sequence and illustrated with simple and accurate analogies.

Second, use concrete language and visual imagery. As Steven Pinker puts it in his book The Sense of Style: “We are primates, with a third of our brains dedicated to vision, and large swaths devoted to touch, hearing, motion, and space. For us to go from “I think I understand” to “I understand,” we need to see the sights and feel the motions.” and he continues “Many experiments have shown that readers understand and remember material far better when it is expressed in concrete language that allows them to form visual images”.

Third, work to improve your writing skills. You could start by replacing linking and light verbs such as ‘to be, is, going, become, bring, make, take, have’, by more precise action verbs. For example, you could say ‘agile’ instead of ‘going fast’. There is plenty of great advice on the internet.

Finally, check for spelling, punctuation and grammatical mistakes. If you can’t do it yourself, use Grammarly and then ask a friend to help you. As we are such a small team, we can’t correct every submission.
6. Is your code well displayed?

Please don’t screenshot your code but use one of these two other solutions:

    Medium’s native code blocks & inline code
    Embed GitHub gists

If you paste a code within your article, be sure to explain it so all your readers can follow what you are doing.
7. Check your facts

Whenever you provide a fact, if it’s not self-evident, let us know where you learned it. Tell us who your sources are and where your data originated. If we want to have a conversation we all need to be on the same page. Maybe something you say will spark a discussion, but if we want to be sure we are not at cross purposes, we need to go back to the original and read for ourselves in case we are missing a vital piece of the puzzle that makes everything you say make sense.
8. Is your conclusion to the point and not promotional?

You can use your conclusion to link the original post and even a few relevant articles but please avoid requesting for claps or having many links to your previously published articles. Your readers know that they can click on your profile to see all your posts. They also know that they can follow you, so there is no need to request it.

For your references, please respect this format:

[X] N. Name, Title (Year), Source

For example, your first reference should look like this:

[1] A. Pesah, A. Wehenkel and G. Louppe, Recurrent Machines for Likelihood-Free Inference (2018), NeurIPS 2018 Workshop on Meta-Learning
9. Are your tags precise enough?

The more specific your tags, the easier it is for readers to find your article and for us to classify and recommend your post to the relevant audience.

Depending on the tags you choose, your post will be featured on our data science, machine learning, programming or visualization page. Please choose precise tags as we can’t allow a post to be in all our categories at once.

We may change one or two tags before publication. We would do this only to keep our different sections relevant to our readers. For instance, we would want to avoid tagging a post on linear regression as “Artificial Intelligence”.
10. Do you have an amazing image?

A great image attracts and stimulates readers. That is why all the best newspapers always display awesome pictures. Here is what you can do to add one to your post:

    Take one yourself. There is a good chance that your phone is good enough to capture a cool image of your surroundings.
    Make a great graph. If your post involves data analysis, spend some time to make one graph truly unique. You can try R, python, D3.js or even Plotly.
    Use Pixabay or Unsplash. All of their content is released under Creative Commons CC0, which means their photos are safe to use without asking for permission or giving credit to the artist — even for commercial purposes.
{% endblock content %}